{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scraping and Filtering Pipeline Documentation\n",
    "\n",
    "## Project Overview\n",
    "This project is a web scraping solution designed to collect, filter, and store job listings from emprego.sapo.pt, focusing on both Lisbon and International IT positions.\n",
    "\n",
    "## Technologies Used\n",
    "- **Python**: Primary programming language\n",
    "- **Selenium**: Web automation and scraping\n",
    "- **BeautifulSoup**: HTML parsing\n",
    "- **Pandas**: Data manipulation and filtering\n",
    "- **JSON**: Data storage format\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Web Scraping\n",
    "- Uses Selenium WebDriver for dynamic page interaction\n",
    "- Handles cookie consent popups automatically\n",
    "- Scrolls pages to load all content\n",
    "- Extracts data from article elements\n",
    "\n",
    "### 2. Data Extraction\n",
    "- Job titles and URLs\n",
    "- Company names (different methods for Lisboa/Internacional)\n",
    "- Job locations\n",
    "- Job descriptions\n",
    "- Employment types\n",
    "- Posting dates\n",
    "\n",
    "### 3. Data Processing\n",
    "- Two-stage filtering process\n",
    "  - Initial filter for basic validation\n",
    "  - Advanced filter to remove duplicates\n",
    "- Text cleaning and normalization\n",
    "- JSON output formatting\n",
    "\n",
    "### 4. File Management\n",
    "- Separate files for Lisboa and Internacional jobs\n",
    "- Automatic file cleanup before new scrapes\n",
    "- UTF-8 encoding support for special characters\n",
    "\n",
    "## Features\n",
    "- **Debug Mode**: Optional browser inspection mode\n",
    "- **Pagination**: Handles multiple pages of results\n",
    "- **Error Handling**: Graceful handling of missing data\n",
    "- **Different HTML Structures**: Adapts to Lisboa vs Internacional layouts\n",
    "\n",
    "## Output Files\n",
    "- `vagas_filtradas.json`: Lisboa job listings\n",
    "- `vagas_filtradas_internacional.json`: International job listings\n",
    "\n",
    "## Performance Considerations\n",
    "- Built-in delays for page loading\n",
    "- Page limit to control scraping scope\n",
    "- Memory-efficient processing with generators\n",
    "\n",
    "## Usage Notes\n",
    "- Set `debug = True` for development/testing\n",
    "- Adjust `limite` parameter to control pages scraped\n",
    "- Chrome WebDriver required for execution\n",
    "- Internet connection required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scraping and Filtering Pipeline\n",
    "This notebook combines web scraping and data filtering for job listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Global Variables\n",
    "vagas = []\n",
    "vagas_finais=[]\n",
    "pagina = 1\n",
    "debug = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function Definitions\n",
    "\n",
    "def filter_jobs(vagas):\n",
    "    if not vagas:  # Check if vagas is empty\n",
    "        return []\n",
    "    return vagas  # Return all jobs without filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_filter(vagas):\n",
    "    if not vagas:  # Check if vagas is empty\n",
    "        return []\n",
    "    df = pd.DataFrame(vagas)\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "    return df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.replace('ï»¿', '')\n",
    "    return ' '.join(text.split()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_data(jobs):\n",
    "    clean_jobs = []\n",
    "    for job in jobs:\n",
    "        clean_job = {\n",
    "            key: clean_text(value) for key, value in job.items()\n",
    "        }\n",
    "        clean_jobs.append(clean_job)\n",
    "    return clean_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(url_base, saida_json, limite=5):\n",
    "    # Initialize local variables\n",
    "    current_page = 1\n",
    "    local_vagas = []\n",
    "    is_international = 'local=Internacional' in url_base\n",
    "\n",
    "    print(f\"\\nğŸ“ Iniciando scraping para {url_base}\")\n",
    "    os.makedirs(os.path.dirname(saida_json), exist_ok=True)\n",
    "\n",
    "    # Setup do navegador\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            url = url_base.format(current_page)\n",
    "            print(f\"\\nğŸŒ PÃ¡gina {current_page}: {url}\")\n",
    "            driver.get(url)\n",
    "\n",
    "            # Handle cookies only on first page\n",
    "            if current_page == 1:\n",
    "                try:\n",
    "                    rejeitar_span = wait.until(EC.element_to_be_clickable(\n",
    "                        (By.XPATH, \"//span[translate(normalize-space(), 'REJEITAR TODOS', 'rejeitar todos') = 'rejeitar todos']\")\n",
    "                    ))\n",
    "                    rejeitar_span.click()\n",
    "                    print(\"âœ… Clicado em 'REJEITAR TODOS'\")\n",
    "                    time.sleep(2)\n",
    "                except TimeoutException:\n",
    "                    print(\"â„¹ï¸ BotÃ£o 'REJEITAR TODOS' nÃ£o apareceu.\")\n",
    "\n",
    "            if debug:\n",
    "                print(\"Debug mode: Keeping browser open for inspection\")\n",
    "                while True:\n",
    "                    time.sleep(1)\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            itens = soup.find_all(\"article\", attrs={\"data-v-37f59e59\": True})\n",
    "            print(f\"ğŸ” {len(itens)} blocos encontrados.\")\n",
    "\n",
    "            for item in itens:\n",
    "                try:\n",
    "                    # Get title and URL\n",
    "                    link = item.find(\"a\", {\"data-trackerlink\": \"offers_list|offer|text_offer\"})\n",
    "                    if not link:\n",
    "                        continue\n",
    "\n",
    "                    titulo = link.get_text(strip=True)\n",
    "                    url = link[\"href\"] if \"href\" in link.attrs else \"\"\n",
    "                    if url and not url.startswith(\"http\"):\n",
    "                        url = \"https://emprego.sapo.pt\" + url\n",
    "\n",
    "                    # Get company name and location based on job type\n",
    "                    info_text = item.get_text(\" \", strip=True)\n",
    "                    \n",
    "                    if is_international:\n",
    "                        # Get company name for international jobs\n",
    "                        company_link = item.find(\"a\", {\"data-trackerlink\": \"offers_list|offer|text_company\"})\n",
    "                        empresa = company_link.get_text(strip=True) if company_link else \"\"\n",
    "\n",
    "                        # Get location for international jobs\n",
    "                        location_text = info_text\n",
    "                        location_start = location_text.find('class=\"location\">') + len('class=\"location\">')\n",
    "                        location_end = location_text.find('</li>', location_start)\n",
    "                        local = location_text[location_start:location_end].strip() if location_start > -1 and location_end > -1 else \"Internacional\"\n",
    "                    else:\n",
    "                        # Existing Lisboa logic\n",
    "                        empresa_match = re.search(rf\"{re.escape(titulo)}\\s*(.*?)\\s*Lisboa\", info_text)\n",
    "                        empresa = empresa_match.group(1).strip() if empresa_match else \"\"\n",
    "                        local = \"Lisboa , Portugal\"\n",
    "\n",
    "                    # Common fields\n",
    "                    tipo = \"Full-Time\" if \"Full-Time\" in info_text else \"\"\n",
    "                    data = \"Ãšltimas 24 horas\" if \"Ãšltimas 24 horas\" in info_text else \"\"\n",
    "\n",
    "                    # Get description\n",
    "                    description_p = item.find(\"p\")\n",
    "                    descricao = description_p.get_text(strip=True) if description_p else \"\"\n",
    "\n",
    "                    if titulo and not titulo.startswith(\"Curso\"):\n",
    "                        vaga = {\n",
    "                            \"titulo\": titulo,\n",
    "                            \"empresa\": empresa,\n",
    "                            \"local\": local,\n",
    "                            \"descricao\": descricao,\n",
    "                            \"tipo\": tipo,\n",
    "                            \"data\": data,\n",
    "                            \"url\": url\n",
    "                        }\n",
    "                        local_vagas.append(vaga)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing item: {e}\")\n",
    "                    continue\n",
    "\n",
    "            current_page += 1\n",
    "            if current_page > limite:  # Limit pages\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        if not debug:\n",
    "            driver.quit()\n",
    "\n",
    "    # Apply filters\n",
    "    vagas_filtradas = filter_jobs(local_vagas)\n",
    "    vagas_finais = advanced_filter(vagas_filtradas)\n",
    "\n",
    "    # Delete existing file if it exists\n",
    "    if os.path.exists(saida_json):\n",
    "        try:\n",
    "            os.remove(saida_json)\n",
    "            print(f\"ğŸ—‘ï¸ Arquivo existente removido: {saida_json}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erro ao remover arquivo: {e}\")\n",
    "\n",
    "    # Save filtered results\n",
    "    with open(saida_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vagas_finais, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nğŸ’¾ Total de {len(vagas_finais)} vagas filtradas salvas em '{saida_json}'\")\n",
    "    return vagas_finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Starting to scrape Lisboa jobs...\n",
      "\n",
      "ğŸ“ Iniciando scraping para https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina={}&ordem=mais-recentes\n",
      "\n",
      "ğŸŒ PÃ¡gina 1: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=1&ordem=mais-recentes\n",
      "\n",
      "ğŸŒ PÃ¡gina 1: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=1&ordem=mais-recentes\n",
      "âœ… Clicado em 'REJEITAR TODOS'\n",
      "âœ… Clicado em 'REJEITAR TODOS'\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 2: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=2&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 2: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=2&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 3: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=3&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 3: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=3&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 4: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=4&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 4: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=4&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 5: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=5&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 5: https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina=5&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "ğŸ” 10 blocos encontrados.\n",
      "ğŸ—‘ï¸ Arquivo existente removido: ./vagas_filtradas.json\n",
      "\n",
      "ğŸ’¾ Total de 45 vagas filtradas salvas em './vagas_filtradas.json'\n",
      "\n",
      "ğŸ’¾ Saved 45 Lisboa jobs to ./vagas_filtradas.json\n",
      "ğŸ—‘ï¸ Arquivo existente removido: ./vagas_filtradas.json\n",
      "\n",
      "ğŸ’¾ Total de 45 vagas filtradas salvas em './vagas_filtradas.json'\n",
      "\n",
      "ğŸ’¾ Saved 45 Lisboa jobs to ./vagas_filtradas.json\n"
     ]
    }
   ],
   "source": [
    "# Process Lisbon Jobs\n",
    "lisboa_url = \"https://emprego.sapo.pt/offers?local=lisboa&categoria=informatica-tecnologias&pagina={}&ordem=mais-recentes\"\n",
    "lisboa_output = \"./vagas_filtradas.json\"\n",
    "\n",
    "# Clear previous data\n",
    "vagas = []\n",
    "vagas_finais = []\n",
    "\n",
    "print(\"\\nğŸ” Starting to scrape Lisboa jobs...\")\n",
    "lisboa_jobs = scrape_jobs(lisboa_url, lisboa_output)\n",
    "\n",
    "# Clean and save Lisboa jobs\n",
    "cleaned_lisboa_jobs = clean_job_data(lisboa_jobs)\n",
    "with open(lisboa_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_lisboa_jobs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saved {len(cleaned_lisboa_jobs)} Lisboa jobs to {lisboa_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Starting to scrape Internacional jobs...\n",
      "\n",
      "ğŸ“ Iniciando scraping para https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina={}&ordem=mais-recentes\n",
      "\n",
      "ğŸŒ PÃ¡gina 1: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=1&ordem=mais-recentes\n",
      "\n",
      "ğŸŒ PÃ¡gina 1: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=1&ordem=mais-recentes\n",
      "âœ… Clicado em 'REJEITAR TODOS'\n",
      "âœ… Clicado em 'REJEITAR TODOS'\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 2: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=2&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 2: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=2&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 3: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=3&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 3: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=3&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 4: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=4&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 4: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=4&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 5: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=5&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "\n",
      "ğŸŒ PÃ¡gina 5: https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina=5&ordem=mais-recentes\n",
      "ğŸ” 10 blocos encontrados.\n",
      "ğŸ” 10 blocos encontrados.\n",
      "ğŸ—‘ï¸ Arquivo existente removido: ./vagas_filtradas_internacional.json\n",
      "\n",
      "ğŸ’¾ Total de 45 vagas filtradas salvas em './vagas_filtradas_internacional.json'\n",
      "\n",
      "ğŸ’¾ Saved 45 Internacional jobs to ./vagas_filtradas_internacional.json\n",
      "ğŸ—‘ï¸ Arquivo existente removido: ./vagas_filtradas_internacional.json\n",
      "\n",
      "ğŸ’¾ Total de 45 vagas filtradas salvas em './vagas_filtradas_internacional.json'\n",
      "\n",
      "ğŸ’¾ Saved 45 Internacional jobs to ./vagas_filtradas_internacional.json\n"
     ]
    }
   ],
   "source": [
    "# Process International Jobs\n",
    "internacional_url = \"https://emprego.sapo.pt/offers?local=Internacional&categoria=informatica-tecnologias&pagina={}&ordem=mais-recentes\"\n",
    "internacional_output = \"./vagas_filtradas_internacional.json\"\n",
    "\n",
    "# Clear previous data\n",
    "vagas = []\n",
    "vagas_finais = []\n",
    "\n",
    "print(\"\\nğŸ” Starting to scrape Internacional jobs...\")\n",
    "internacional_jobs = scrape_jobs(internacional_url, internacional_output)\n",
    "\n",
    "# Clean and save Internacional jobs\n",
    "cleaned_internacional_jobs = clean_job_data(internacional_jobs)\n",
    "with open(internacional_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_internacional_jobs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saved {len(cleaned_internacional_jobs)} Internacional jobs to {internacional_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
